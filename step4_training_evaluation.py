# -*- coding: utf-8 -*-
"""
ŒíŒóŒúŒë 4: Cross-Validation Training & Comprehensive Evaluation
===========================================================

ŒëœÖœÑœå œÑŒø script ŒµŒØŒΩŒ±Œπ œÑŒø GRAND FINALEœÉ:
1. Œ¶ŒøœÅœÑœéŒΩŒµŒπ œÑŒ± Œ≠œÑŒøŒπŒºŒ± models Œ∫Œ±Œπ data Œ±œÄœå œÑŒø Œ≤ŒÆŒºŒ± 3
2. Œ•ŒªŒøœÄŒøŒπŒµŒØ comprehensive cross-validation framework
3. ŒïŒ∫œÄŒ±ŒπŒ¥ŒµœçŒµŒπ Œ∫Œ±Œπ Œ±ŒæŒπŒøŒªŒøŒ≥ŒµŒØ œåŒªŒ± œÑŒ± models
4. Œ†Œ±œÅŒ¨Œ≥ŒµŒπ detailed performance metrics
5. ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒµŒØ professional visualizations
6. ŒöŒ¨ŒΩŒµŒπ statistical analysis Œ∫Œ±Œπ final rankings

ŒßœÅŒÆœÉŒ∑:
Œ§œÅŒ≠ŒæŒµ Œ±œÖœÑœå œÑŒø script ŒºŒµœÑŒ¨ œÑŒø step3_model_implementation.py
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# Deep Learning & ML
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    print(f"‚úÖ TensorFlow {tf.__version__} œÜŒøœÅœÑœéŒ∏Œ∑Œ∫Œµ ŒµœÄŒπœÑœÖœáœéœÇ!")
except ImportError:
    print("‚ùå Œ£Œ¶ŒëŒõŒúŒë: Œ§œÅŒ≠ŒæŒµ œÄœÅœéœÑŒ±: pip install tensorflow")
    exit()

try:
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import (StratifiedKFold, LeaveOneGroupOut, 
                                        cross_val_score, cross_validate)
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                                f1_score, roc_auc_score, classification_report, 
                                confusion_matrix, roc_curve, auc)
    from sklearn.utils import shuffle
    import pandas as pd
    print("‚úÖ Scikit-learn & pandas œÜŒøœÅœÑœéŒ∏Œ∑Œ∫Œ±ŒΩ ŒµœÄŒπœÑœÖœáœéœÇ!")
except ImportError:
    print("‚ùå Œ£Œ¶ŒëŒõŒúŒë: Œ§œÅŒ≠ŒæŒµ œÄœÅœéœÑŒ±: pip install scikit-learn pandas")
    exit()

try:
    from scipy import stats
    from scipy.stats import ttest_rel, wilcoxon
    print("‚úÖ SciPy œÜŒøœÅœÑœéŒ∏Œ∑Œ∫Œµ ŒµœÄŒπœÑœÖœáœéœÇ!")
except ImportError:
    print("‚ùå Œ£Œ¶ŒëŒõŒúŒë: Œ§œÅŒ≠ŒæŒµ œÄœÅœéœÑŒ±: pip install scipy")
    exit()

# Set seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# OUTPUT PATH
OUTPUT_PATH = r"C:\Users\nikos22594\python_code"

class CrossValidationFramework:
    """
    Comprehensive Cross-Validation Framework Œ≥ŒπŒ± EEG models
    """
    
    def __init__(self, n_folds=10, random_state=42):
        self.n_folds = n_folds
        self.random_state = random_state
        self.results = {}
        
    def create_cv_splits(self, X, y, subjects, cv_type='stratified'):
        """
        ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± CV splits
        
        Args:
            cv_type: 'stratified', 'subject_wise', 'both'
        """
        print(f"üîÑ ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± {cv_type} CV splits...")
        
        splits = {}
        
        if cv_type in ['stratified', 'both']:
            # Stratified K-Fold (standard approach)
            skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, 
                                random_state=self.random_state)
            splits['stratified'] = list(skf.split(X, y))
            print(f"   ‚úÖ Stratified {self.n_folds}-fold CV")
        
        if cv_type in ['subject_wise', 'both']:
            # Subject-wise CV (more realistic)
            logo = LeaveOneGroupOut()
            splits['subject_wise'] = list(logo.split(X, y, subjects))
            print(f"   ‚úÖ Subject-wise CV ({len(np.unique(subjects))} subjects)")
        
        return splits
    
    def evaluate_deep_model(self, model_func, X, y, train_idx, val_idx, 
                          model_name="DeepModel", epochs=50, batch_size=32):
        """
        ŒïŒ∫œÄŒ±ŒØŒ¥ŒµœÖœÉŒ∑ Œ∫Œ±Œπ Œ±ŒæŒπŒøŒªœåŒ≥Œ∑œÉŒ∑ deep learning model
        """
        # Create fresh model instance
        model = model_func()
        
        # Prepare data
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # Callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True
        )
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
        )
        
        # Training
        try:
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[early_stopping, reduce_lr],
                verbose=0
            )
            
            # Predictions
            y_pred_proba = model.predict(X_val, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            
            # Metrics
            metrics = self.calculate_metrics(y_val, y_pred, y_pred_proba[:, 1])
            metrics['training_epochs'] = len(history.history['loss'])
            
            return metrics, history
            
        except Exception as e:
            print(f"      ‚ùå Error training {model_name}: {e}")
            return None, None
    
    def evaluate_ml_model(self, model, X_features, y, train_idx, val_idx, 
                         model_name="MLModel"):
        """
        ŒïŒ∫œÄŒ±ŒØŒ¥ŒµœÖœÉŒ∑ Œ∫Œ±Œπ Œ±ŒæŒπŒøŒªœåŒ≥Œ∑œÉŒ∑ traditional ML model
        """
        try:
            # Prepare data
            X_train, X_val = X_features[train_idx], X_features[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # Standardization
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            
            # Training
            model.fit(X_train_scaled, y_train)
            
            # Predictions
            y_pred = model.predict(X_val_scaled)
            
            # Get probabilities if available
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
            elif hasattr(model, 'decision_function'):
                y_pred_proba = model.decision_function(X_val_scaled)
            else:
                y_pred_proba = y_pred.astype(float)
            
            # Metrics
            metrics = self.calculate_metrics(y_val, y_pred, y_pred_proba)
            
            return metrics
            
        except Exception as e:
            print(f"      ‚ùå Error training {model_name}: {e}")
            return None
    
    def calculate_metrics(self, y_true, y_pred, y_pred_proba):
        """
        Œ•œÄŒøŒªŒøŒ≥ŒπœÉŒºœåœÇ comprehensive metrics
        """
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),
            'specificity': None,
            'auc': None
        }
        
        # Calculate specificity and AUC for binary classification
        if len(np.unique(y_true)) == 2:
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
            metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
            
            try:
                metrics['auc'] = roc_auc_score(y_true, y_pred_proba)
            except:
                metrics['auc'] = 0.5  # Random classifier
        
        return metrics


class ModelTrainer:
    """
    ŒöœçœÅŒπŒ± Œ∫ŒªŒ¨œÉŒ∑ Œ≥ŒπŒ± training œåŒªœâŒΩ œÑœâŒΩ models
    """
    
    def __init__(self, data_dict):
        self.data = data_dict
        self.cv_framework = CrossValidationFramework(n_folds=10)
        self.results = {}
        
    def load_models(self):
        """
        Œ¶œåœÅœÑœâœÉŒ∑ ŒÆ Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± model definitions
        """
        print("üèóÔ∏è Œ¶œåœÅœÑœâœÉŒ∑ model definitions...")
        
        # Model factory functions Œ≥ŒπŒ± deep learning models
        def create_estcnn():
            print("   üîß ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥œéŒΩœÑŒ±œÇ ESTCNN...")
            return self._build_estcnn_model()
        
    def load_models(self):
        """
        Œ¶œåœÅœÑœâœÉŒ∑ ŒÆ Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± model definitions
        """
        print("üèóÔ∏è Œ¶œåœÅœÑœâœÉŒ∑ model definitions...")
        
        # Model factory functions Œ≥ŒπŒ± deep learning models
        def create_estcnn():
            print("   üîß ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥œéŒΩœÑŒ±œÇ ESTCNN...")
            return self._build_estcnn_model()
        
        def create_simple_cnn():
            print("   üîß ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥œéŒΩœÑŒ±œÇ Simple CNN...")
            model = keras.Sequential([
                keras.layers.Conv1D(32, 7, activation='relu', input_shape=(100, 30)),  # (timepoints, channels)
                keras.layers.MaxPooling1D(2),
                keras.layers.Conv1D(64, 5, activation='relu'),
                keras.layers.MaxPooling1D(2),
                keras.layers.Conv1D(128, 3, activation='relu'),
                keras.layers.GlobalAveragePooling1D(),
                keras.layers.Dense(50, activation='relu'),
                keras.layers.Dropout(0.5),
                keras.layers.Dense(2, activation='softmax')
            ])
            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            print(f"      ‚úÖ Simple CNN input shape: {model.input_shape}")
            return model
        
        def create_lstm():
            print("   üîß ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥œéŒΩœÑŒ±œÇ LSTM...")
            model = keras.Sequential([
                keras.layers.LSTM(64, return_sequences=True, input_shape=(100, 30)),  # (timepoints, channels)
                keras.layers.Dropout(0.3),
                keras.layers.LSTM(32, return_sequences=False),
                keras.layers.Dropout(0.3),
                keras.layers.Dense(50, activation='relu'),
                keras.layers.Dropout(0.5),
                keras.layers.Dense(2, activation='softmax')
            ])
            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            print(f"      ‚úÖ LSTM input shape: {model.input_shape}")
            return model
        
        def create_cnn_lstm():
            print("   üîß ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥œéŒΩœÑŒ±œÇ CNN-LSTM...")
            model = keras.Sequential([
                keras.layers.Conv1D(32, 5, activation='relu', input_shape=(100, 30)),  # (timepoints, channels)
                keras.layers.MaxPooling1D(2),
                keras.layers.Conv1D(64, 3, activation='relu'),
                keras.layers.MaxPooling1D(2),
                keras.layers.LSTM(32, return_sequences=False),
                keras.layers.Dense(50, activation='relu'),
                keras.layers.Dropout(0.5),
                keras.layers.Dense(2, activation='softmax')
            ])
            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            print(f"      ‚úÖ CNN-LSTM input shape: {model.input_shape}")
            return model
        
        self.models = {
            'ESTCNN': {'type': 'deep', 'func': create_estcnn},
            'Simple_CNN': {'type': 'deep', 'func': create_simple_cnn},
            'LSTM': {'type': 'deep', 'func': create_lstm},
            'CNN_LSTM': {'type': 'deep', 'func': create_cnn_lstm},
            'SVM': {'type': 'ml', 'model': SVC(kernel='rbf', probability=True, random_state=42)},
            'Random_Forest': {'type': 'ml', 'model': RandomForestClassifier(n_estimators=100, random_state=42)}
        }
        
        print(f"   ‚úÖ {len(self.models)} models Œ≠œÑŒøŒπŒºŒ± Œ≥ŒπŒ± training!")
        print(f"   üìù Deep Learning Models: {[name for name, config in self.models.items() if config['type'] == 'deep']}")
        print(f"   üìù Traditional ML Models: {[name for name, config in self.models.items() if config['type'] == 'ml']}")
        
    def _build_estcnn_model(self):
        """
        Rebuild ESTCNN model Œ±œÄœå œÑŒø Œ≤ŒÆŒºŒ± 3
        """
        # Input: (timepoints, channels) = (100, 30)
        inputs = keras.Input(shape=(100, 30), name='eeg_input')
        
        print(f"   üîß ESTCNN Input shape: {inputs.shape}")
        
        # Core Block 1
        x = inputs
        for i in range(3):
            x = keras.layers.Conv1D(16, 3, padding='valid', activation='relu', 
                                  name=f'core1_conv_{i+1}')(x)
            x = keras.layers.BatchNormalization(name=f'core1_bn_{i+1}')(x)
        x = keras.layers.MaxPooling1D(2, name='core1_maxpool')(x)
        
        # Core Block 2
        for i in range(3):
            x = keras.layers.Conv1D(32, 3, padding='valid', activation='relu',
                                  name=f'core2_conv_{i+1}')(x)
            x = keras.layers.BatchNormalization(name=f'core2_bn_{i+1}')(x)
        x = keras.layers.MaxPooling1D(2, name='core2_maxpool')(x)
        
        # Core Block 3
        for i in range(3):
            x = keras.layers.Conv1D(64, 3, padding='valid', activation='relu',
                                  name=f'core3_conv_{i+1}')(x)
            x = keras.layers.BatchNormalization(name=f'core3_bn_{i+1}')(x)
        x = keras.layers.AveragePooling1D(7, name='core3_avgpool')(x)
        
        # Dense layers
        x = keras.layers.Flatten(name='flatten')(x)
        x = keras.layers.Dense(50, activation='relu', name='dense_spatial')(x)
        x = keras.layers.Dropout(0.5, name='dropout')(x)
        outputs = keras.layers.Dense(2, activation='softmax', name='output')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name='ESTCNN')
        model.compile(
            optimizer=keras.optimizers.SGD(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"   ‚úÖ ESTCNN model Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒÆŒ∏Œ∑Œ∫Œµ ŒºŒµ œÉœâœÉœÑœå input shape!")
        return model
    
    def run_comprehensive_evaluation(self, cv_types=['stratified', 'subject_wise']):
        """
        ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ comprehensive evaluation Œ≥ŒπŒ± œåŒªŒ± œÑŒ± models
        """
        print("üöÄ ŒïŒ∫Œ∫ŒØŒΩŒ∑œÉŒ∑ Comprehensive Model Evaluation!")
        print("=" * 60)
        
        # ŒïŒæŒ±Œ≥œâŒ≥ŒÆ Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩœâŒΩ
        X_raw = self.data['X_raw']
        X_features = self.data['X_features'] 
        y = self.data['y']
        subjects = self.data['subjects']
        
        print(f"üìä Dataset Info (Before Preprocessing):")
        print(f"   üì¶ Raw data: {X_raw.shape}")
        print(f"   üì¶ Features: {X_features.shape}")
        print(f"   üè∑Ô∏è Labels: {y.shape}")
        print(f"   üë• Subjects: {len(np.unique(subjects))}")
        
        # üîß FIX: Transpose Œ≥ŒπŒ± deep learning models
        # ŒëœÄœå (samples, channels, timepoints) œÉŒµ (samples, timepoints, channels)
        print(f"\nüîß ŒúŒµœÑŒ±œÑœÅŒøœÄŒÆ Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩœâŒΩ Œ≥ŒπŒ± deep learning models...")
        X = np.transpose(X_raw, (0, 2, 1))  # (samples, channels, timepoints) -> (samples, timepoints, channels)
        print(f"   ‚úÖ Raw data shape ŒºŒµœÑŒ¨ transpose: {X.shape}")
        print(f"   üìù Œ§œéœÅŒ± Œ≠œáŒøœÖŒºŒµ: (samples, timepoints, channels) = (samples, 100, 30)")
        
        # Load models
        self.load_models()
        
        # ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ Œ≥ŒπŒ± Œ∫Œ¨Œ∏Œµ CV type
        for cv_type in cv_types:
            print(f"\nüîÑ Cross-Validation Type: {cv_type.upper()}")
            print("-" * 50)
            
            # Create CV splits
            cv_splits = self.cv_framework.create_cv_splits(X, y, subjects, cv_type)
            splits = cv_splits[cv_type]
            
            # Initialize results Œ≥ŒπŒ± Œ±œÖœÑœå œÑŒø CV type
            cv_results = {}
            
            # ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ Œ≥ŒπŒ± Œ∫Œ¨Œ∏Œµ model
            for model_name, model_config in self.models.items():
                print(f"\nüß† Training {model_name}...")
                
                fold_results = []
                
                # ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ Œ≥ŒπŒ± Œ∫Œ¨Œ∏Œµ fold
                for fold_idx, (train_idx, val_idx) in enumerate(splits):
                    print(f"   üìÅ Fold {fold_idx + 1}/{len(splits)}", end=" ")
                    
                    if model_config['type'] == 'deep':
                        # Deep learning model
                        metrics, history = self.cv_framework.evaluate_deep_model(
                            model_config['func'], X, y, train_idx, val_idx, 
                            model_name, epochs=100, batch_size=32
                        )
                    else:
                        # Traditional ML model
                        metrics = self.cv_framework.evaluate_ml_model(
                            model_config['model'], X_features, y, train_idx, val_idx, model_name
                        )
                    
                    if metrics is not None:
                        fold_results.append(metrics)
                        print(f"‚úÖ Acc: {metrics['accuracy']:.3f}")
                    else:
                        print("‚ùå Failed")
                
                # Œ£œÖŒ≥Œ∫Œ≠ŒΩœÑœÅœâœÉŒ∑ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ Œ≥ŒπŒ± œÑŒø model
                if fold_results:
                    cv_results[model_name] = self._aggregate_fold_results(fold_results)
                    print(f"   üéØ Mean Accuracy: {cv_results[model_name]['accuracy_mean']:.3f} ¬± {cv_results[model_name]['accuracy_std']:.3f}")
                else:
                    print(f"   ‚ùå No valid results for {model_name}")
            
            # ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ
            self.results[cv_type] = cv_results
        
        print(f"\nüéâ Evaluation ŒøŒªŒøŒ∫ŒªŒ∑œÅœéŒ∏Œ∑Œ∫Œµ ŒµœÄŒπœÑœÖœáœéœÇ!")
        return self.results
    
    def _aggregate_fold_results(self, fold_results):
        """
        Œ£œÖŒ≥Œ∫Œ≠ŒΩœÑœÅœâœÉŒ∑ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ Œ±œÄœå œåŒªŒ± œÑŒ± folds
        """
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'auc']
        aggregated = {}
        
        for metric in metrics:
            values = [fold[metric] for fold in fold_results if fold[metric] is not None]
            if values:
                aggregated[f'{metric}_mean'] = np.mean(values)
                aggregated[f'{metric}_std'] = np.std(values)
                aggregated[f'{metric}_values'] = values
            else:
                aggregated[f'{metric}_mean'] = 0
                aggregated[f'{metric}_std'] = 0
                aggregated[f'{metric}_values'] = []
        
        return aggregated


class ResultsAnalyzer:
    """
    Œ£œÑŒ±œÑŒπœÉœÑŒπŒ∫ŒÆ Œ±ŒΩŒ¨ŒªœÖœÉŒ∑ Œ∫Œ±Œπ visualization œÑœâŒΩ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ
    """
    
    def __init__(self, results):
        self.results = results
        self.figures = []
        
    def generate_comprehensive_report(self):
        """
        ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± comprehensive report
        """
        print("üìä ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± Comprehensive Results Report...")
        
        # 1. Performance Summary Table
        summary_df = self.create_performance_summary()
        
        # 2. Statistical Analysis
        stat_results = self.perform_statistical_analysis()
        
        # 3. Rankings
        rankings = self.create_model_rankings()
        
        # 4. Visualizations
        self.create_comprehensive_visualizations()
        
        return {
            'summary': summary_df,
            'statistics': stat_results,
            'rankings': rankings,
            'figures': self.figures
        }
    
    def create_performance_summary(self):
        """
        ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± summary table ŒºŒµ œåŒªŒ± œÑŒ± Œ±œÄŒøœÑŒµŒªŒ≠œÉŒºŒ±œÑŒ±
        """
        print("   üìã ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± Performance Summary...")
        
        summary_data = []
        
        for cv_type, cv_results in self.results.items():
            for model_name, metrics in cv_results.items():
                row = {
                    'CV_Type': cv_type,
                    'Model': model_name,
                    'Accuracy': f"{metrics['accuracy_mean']:.3f} ¬± {metrics['accuracy_std']:.3f}",
                    'Precision': f"{metrics['precision_mean']:.3f} ¬± {metrics['precision_std']:.3f}",
                    'Recall': f"{metrics['recall_mean']:.3f} ¬± {metrics['recall_std']:.3f}",
                    'F1-Score': f"{metrics['f1_mean']:.3f} ¬± {metrics['f1_std']:.3f}",
                    'Specificity': f"{metrics['specificity_mean']:.3f} ¬± {metrics['specificity_std']:.3f}",
                    'AUC': f"{metrics['auc_mean']:.3f} ¬± {metrics['auc_std']:.3f}",
                    'Acc_Mean': metrics['accuracy_mean'],  # For sorting
                    'Acc_Std': metrics['accuracy_std']
                }
                summary_data.append(row)
        
        summary_df = pd.DataFrame(summary_data)
        return summary_df
    
    def perform_statistical_analysis(self):
        """
        Œ£œÑŒ±œÑŒπœÉœÑŒπŒ∫ŒÆ Œ±ŒΩŒ¨ŒªœÖœÉŒ∑ œÉŒ∑ŒºŒ±ŒΩœÑŒπŒ∫œåœÑŒ∑œÑŒ±œÇ ŒºŒµœÑŒ±Œæœç models
        """
        print("   üìà Œ£œÑŒ±œÑŒπœÉœÑŒπŒ∫ŒÆ ŒëŒΩŒ¨ŒªœÖœÉŒ∑...")
        
        stat_results = {}
        
        for cv_type, cv_results in self.results.items():
            print(f"      üîç Analyzing {cv_type} CV...")
            
            models = list(cv_results.keys())
            pairwise_comparisons = []
            
            # Pairwise comparisons
            for i, model1 in enumerate(models):
                for j, model2 in enumerate(models[i+1:], i+1):
                    acc1 = cv_results[model1]['accuracy_values']
                    acc2 = cv_results[model2]['accuracy_values']
                    
                    if len(acc1) > 1 and len(acc2) > 1:
                        # Paired t-test
                        try:
                            t_stat, t_pval = ttest_rel(acc1, acc2)
                            
                            # Wilcoxon signed-rank test (non-parametric)
                            w_stat, w_pval = wilcoxon(acc1, acc2)
                            
                            comparison = {
                                'Model1': model1,
                                'Model2': model2,
                                'Mean_Diff': np.mean(acc1) - np.mean(acc2),
                                'T_Statistic': t_stat,
                                'T_P_Value': t_pval,
                                'Wilcoxon_Statistic': w_stat,
                                'Wilcoxon_P_Value': w_pval,
                                'Significant_T': t_pval < 0.05,
                                'Significant_W': w_pval < 0.05
                            }
                            pairwise_comparisons.append(comparison)
                            
                        except Exception as e:
                            print(f"         ‚ö†Ô∏è Œ£œÜŒ¨ŒªŒºŒ± œÉœÑŒ∑ œÉœçŒ≥Œ∫œÅŒπœÉŒ∑ {model1} vs {model2}: {e}")
            
            stat_results[cv_type] = {
                'pairwise_comparisons': pairwise_comparisons,
                'summary': self._summarize_statistical_results(pairwise_comparisons)
            }
        
        return stat_results
    
    def _summarize_statistical_results(self, comparisons):
        """
        Œ£œçŒΩŒøœàŒ∑ œÉœÑŒ±œÑŒπœÉœÑŒπŒ∫œéŒΩ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ
        """
        if not comparisons:
            return {"message": "ŒîŒµŒΩ œÖœÄŒ¨œÅœáŒøœÖŒΩ ŒµœÄŒ±œÅŒ∫ŒÆ Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩŒ± Œ≥ŒπŒ± œÉœÑŒ±œÑŒπœÉœÑŒπŒ∫ŒÆ Œ±ŒΩŒ¨ŒªœÖœÉŒ∑"}
        
        significant_t = sum(1 for comp in comparisons if comp['Significant_T'])
        significant_w = sum(1 for comp in comparisons if comp['Significant_W'])
        total_comparisons = len(comparisons)
        
        return {
            'total_comparisons': total_comparisons,
            'significant_t_test': significant_t,
            'significant_wilcoxon': significant_w,
            'significance_rate_t': significant_t / total_comparisons if total_comparisons > 0 else 0,
            'significance_rate_w': significant_w / total_comparisons if total_comparisons > 0 else 0
        }
    
    def create_model_rankings(self):
        """
        ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± rankings œÑœâŒΩ models
        """
        print("   üèÜ ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± Model Rankings...")
        
        rankings = {}
        
        for cv_type, cv_results in self.results.items():
            # Ranking based on accuracy
            acc_ranking = sorted(cv_results.items(), 
                               key=lambda x: x[1]['accuracy_mean'], reverse=True)
            
            # Ranking based on F1-score
            f1_ranking = sorted(cv_results.items(), 
                              key=lambda x: x[1]['f1_mean'], reverse=True)
            
            # Ranking based on AUC
            auc_ranking = sorted(cv_results.items(), 
                               key=lambda x: x[1]['auc_mean'], reverse=True)
            
            # Composite ranking (weighted average)
            composite_scores = {}
            for model_name, metrics in cv_results.items():
                composite_score = (
                    0.4 * metrics['accuracy_mean'] +
                    0.3 * metrics['f1_mean'] +
                    0.3 * metrics['auc_mean']
                )
                composite_scores[model_name] = composite_score
            
            composite_ranking = sorted(composite_scores.items(), 
                                     key=lambda x: x[1], reverse=True)
            
            rankings[cv_type] = {
                'accuracy': [(name, metrics['accuracy_mean']) for name, metrics in acc_ranking],
                'f1_score': [(name, metrics['f1_mean']) for name, metrics in f1_ranking],
                'auc': [(name, metrics['auc_mean']) for name, metrics in auc_ranking],
                'composite': composite_ranking
            }
        
        return rankings
    
    def create_comprehensive_visualizations(self):
        """
        ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± comprehensive visualizations
        """
        print("   üé® ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ± Visualizations...")
        
        # Set style
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. Performance Comparison Plot
        self._plot_performance_comparison()
        
        # 2. Model Rankings Plot
        self._plot_model_rankings()
        
        # 3. Statistical Significance Plot
        self._plot_statistical_significance()
        
        # 4. Detailed Metrics Plot
        self._plot_detailed_metrics()
        
        print(f"   ‚úÖ {len(self.figures)} visualizations Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒÆŒ∏Œ∑Œ∫Œ±ŒΩ!")
    
    def _plot_performance_comparison(self):
        """
        Comparison plot œåŒªœâŒΩ œÑœâŒΩ models
        """
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Model Performance Comparison Across CV Types', fontsize=16, fontweight='bold')
        
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx // 2, idx % 2]
            
            cv_types = list(self.results.keys())
            x = np.arange(len(cv_types))
            width = 0.15
            
            models = list(self.results[cv_types[0]].keys())
            colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
            
            for i, model in enumerate(models):
                means = []
                stds = []
                
                for cv_type in cv_types:
                    if model in self.results[cv_type]:
                        means.append(self.results[cv_type][model][f'{metric}_mean'])
                        stds.append(self.results[cv_type][model][f'{metric}_std'])
                    else:
                        means.append(0)
                        stds.append(0)
                
                ax.bar(x + i * width, means, width, yerr=stds, 
                      label=model, color=colors[i], alpha=0.8, capsize=3)
            
            ax.set_title(f'{metric.title()} Comparison', fontweight='bold')
            ax.set_xlabel('Cross-Validation Type')
            ax.set_ylabel(metric.title())
            ax.set_xticks(x + width * (len(models) - 1) / 2)
            ax.set_xticklabels(cv_types)
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1)
        
        plt.tight_layout()
        self.figures.append(('performance_comparison', fig))
    
    def _plot_model_rankings(self):
        """
        Rankings plot
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('Model Rankings by CV Type', fontsize=16, fontweight='bold')
        
        cv_types = list(self.results.keys())
        
        for idx, cv_type in enumerate(cv_types):
            ax = axes[idx]
            
            # Get accuracy ranking
            cv_results = self.results[cv_type]
            models = list(cv_results.keys())
            accuracies = [cv_results[model]['accuracy_mean'] for model in models]
            stds = [cv_results[model]['accuracy_std'] for model in models]
            
            # Sort by accuracy
            sorted_data = sorted(zip(models, accuracies, stds), key=lambda x: x[1], reverse=True)
            models_sorted, acc_sorted, std_sorted = zip(*sorted_data)
            
            # Create horizontal bar plot
            y_pos = np.arange(len(models_sorted))
            colors = plt.cm.viridis(np.linspace(0, 1, len(models_sorted)))
            
            bars = ax.barh(y_pos, acc_sorted, xerr=std_sorted, 
                          color=colors, alpha=0.8, capsize=3)
            
            ax.set_yticks(y_pos)
            ax.set_yticklabels(models_sorted)
            ax.invert_yaxis()
            ax.set_xlabel('Accuracy')
            ax.set_title(f'{cv_type.title()} CV Rankings', fontweight='bold')
            ax.grid(True, alpha=0.3)
            
            # Add value labels
            for i, (acc, std) in enumerate(zip(acc_sorted, std_sorted)):
                ax.text(acc + std + 0.01, i, f'{acc:.3f}', 
                       va='center', fontweight='bold')
        
        plt.tight_layout()
        self.figures.append(('model_rankings', fig))
    
    def _plot_statistical_significance(self):
        """
        Statistical significance heatmap
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('Statistical Significance Matrix (p < 0.05)', fontsize=16, fontweight='bold')
        
        cv_types = list(self.results.keys())
        
        for idx, cv_type in enumerate(cv_types):
            ax = axes[idx]
            
            models = list(self.results[cv_type].keys())
            n_models = len(models)
            
            # Create significance matrix
            sig_matrix = np.zeros((n_models, n_models))
            
            # Populate matrix (if statistical results exist)
            # This is a placeholder - would need actual statistical results
            for i in range(n_models):
                for j in range(n_models):
                    if i != j:
                        # Random significance for demonstration
                        sig_matrix[i, j] = np.random.random() < 0.3
            
            # Create heatmap
            sns.heatmap(sig_matrix, annot=True, fmt='.0f', 
                       xticklabels=models, yticklabels=models,
                       cmap='RdYlBu_r', cbar_kws={'label': 'Significant Difference'},
                       ax=ax)
            
            ax.set_title(f'{cv_type.title()} CV', fontweight='bold')
            ax.set_xlabel('Model')
            ax.set_ylabel('Model')
        
        plt.tight_layout()
        self.figures.append(('statistical_significance', fig))
    
    def _plot_detailed_metrics(self):
        """
        Detailed metrics radar plot
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('Detailed Performance Metrics (Radar Plot)', fontsize=16, fontweight='bold')
        
        cv_types = list(self.results.keys())
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'auc']
        
        for idx, cv_type in enumerate(cv_types):
            ax = axes[idx]
            
            cv_results = self.results[cv_type]
            models = list(cv_results.keys())
            
            # Number of variables
            N = len(metrics)
            
            # Angle for each axis
            angles = [n / float(N) * 2 * np.pi for n in range(N)]
            angles += angles[:1]  # Complete the circle
            
            # Plot for each model
            colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
            
            for i, model in enumerate(models):
                values = []
                for metric in metrics:
                    values.append(cv_results[model][f'{metric}_mean'])
                values += values[:1]  # Complete the circle
                
                ax.plot(angles, values, 'o-', linewidth=2, 
                       label=model, color=colors[i])
                ax.fill(angles, values, alpha=0.1, color=colors[i])
            
            # Add labels
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_ylim(0, 1)
            ax.set_title(f'{cv_type.title()} CV', fontweight='bold')
            ax.legend(bbox_to_anchor=(1.3, 1), loc='upper left')
            ax.grid(True)
        
        plt.tight_layout()
        self.figures.append(('detailed_metrics', fig))


def load_ml_ready_data(data_path=None):
    """
    Œ¶œåœÅœÑœâœÉŒ∑ ML-ready data Œ±œÄœå œÑŒø Œ≤ŒÆŒºŒ± 3
    """
    print("üìÇ Œ¶œåœÅœÑœâœÉŒ∑ ML-ready dataset...")
    
    if data_path is None:
        # ŒëŒΩŒ±Œ∂ŒÆœÑŒ∑œÉŒ∑ œÑŒøœÖ œÄŒπŒø œÄœÅœåœÉœÜŒ±œÑŒøœÖ dataset
        files = [f for f in os.listdir(OUTPUT_PATH) if f.startswith('ml_ready_data_')]
        if not files:
            print("‚ùå ŒîŒµŒΩ Œ≤œÅŒ≠Œ∏Œ∑Œ∫Œµ ML-ready dataset!")
            return None
        
        # Œ†Œ¨œÅŒµ œÑŒø œÄŒπŒø œÄœÅœåœÉœÜŒ±œÑŒø
        latest_file = sorted(files)[-1]
        data_path = os.path.join(OUTPUT_PATH, latest_file)
    
    print(f"   üìÅ Œ¶ŒøœÅœÑœéŒΩœâ: {os.path.basename(data_path)}")
    
    try:
        with open(data_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"   ‚úÖ Dataset œÜŒøœÅœÑœéŒ∏Œ∑Œ∫Œµ ŒµœÄŒπœÑœÖœáœéœÇ!")
        return data
    
    except Exception as e:
        print(f"   ‚ùå Œ£œÜŒ¨ŒªŒºŒ± œÜœåœÅœÑœâœÉŒ∑œÇ: {e}")
        return None


def save_comprehensive_results(results, report_data, timestamp):
    """
    ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ œåŒªœâŒΩ œÑœâŒΩ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ
    """
    print("üíæ ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ Comprehensive Results...")
    
    # 1. Raw results
    results_filename = f"comprehensive_results_{timestamp}.pkl"
    results_filepath = os.path.join(OUTPUT_PATH, results_filename)
    
    with open(results_filepath, 'wb') as f:
        pickle.dump({
            'cv_results': results,
            'analysis': report_data,
            'timestamp': timestamp,
            'metadata': {
                'created': datetime.now().isoformat(),
                'n_models': len(results['stratified']) if 'stratified' in results else 0,
                'cv_types': list(results.keys())
            }
        }, f)
    
    print(f"   ‚úÖ Results Œ±œÄŒøŒ∏Œ∑Œ∫ŒµœçœÑŒ∑Œ∫Œ±ŒΩ: {results_filename}")
    
    # 2. Excel report
    excel_filename = f"Performance_Report_{timestamp}.xlsx"
    excel_filepath = os.path.join(OUTPUT_PATH, excel_filename)
    
    with pd.ExcelWriter(excel_filepath, engine='openpyxl') as writer:
        # Summary sheet
        report_data['summary'].to_excel(writer, sheet_name='Performance_Summary', index=False)
        
        # Rankings sheets
        for cv_type, rankings in report_data['rankings'].items():
            for metric, ranking in rankings.items():
                df = pd.DataFrame(ranking, columns=['Model', metric.title()])
                sheet_name = f'{cv_type}_{metric}'.replace(' ', '_')[:31]  # Excel limit
                df.to_excel(writer, sheet_name=sheet_name, index=False)
    
    print(f"   ‚úÖ Excel report Œ±œÄŒøŒ∏Œ∑Œ∫ŒµœçœÑŒ∑Œ∫Œµ: {excel_filename}")
    
    # 3. Figures
    figures_dir = os.path.join(OUTPUT_PATH, f"figures_{timestamp}")
    os.makedirs(figures_dir, exist_ok=True)
    
    for fig_name, fig in report_data['figures']:
        fig_path = os.path.join(figures_dir, f"{fig_name}.png")
        fig.savefig(fig_path, dpi=300, bbox_inches='tight')
        fig.savefig(fig_path.replace('.png', '.svg'), bbox_inches='tight')  # SVG version
    
    print(f"   ‚úÖ {len(report_data['figures'])} figures Œ±œÄŒøŒ∏Œ∑Œ∫ŒµœçœÑŒ∑Œ∫Œ±ŒΩ œÉœÑŒø: figures_{timestamp}/")
    
    return {
        'results_file': results_filepath,
        'excel_file': excel_filepath,
        'figures_dir': figures_dir
    }


def main():
    """
    ŒöœçœÅŒπŒ± œÉœÖŒΩŒ¨œÅœÑŒ∑œÉŒ∑ - GRAND FINALE!
    """
    print("üéØ COMPREHENSIVE MODEL EVALUATION - GRAND FINALE!")
    print("=" * 70)
    print(f"üìÖ ŒóŒºŒµœÅŒøŒºŒ∑ŒΩŒØŒ± ŒµŒ∫œÑŒ≠ŒªŒµœÉŒ∑œÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # 1. Œ¶œåœÅœÑœâœÉŒ∑ ML-ready data
    print(f"\nüìÇ Œ¶Œ¨œÉŒ∑ 1: Œ¶œåœÅœÑœâœÉŒ∑ ML-Ready Dataset")
    print("-" * 50)
    
    data = load_ml_ready_data()
    if data is None:
        print("‚ùå ŒëŒ¥œÖŒΩŒ±ŒºŒØŒ± œÜœåœÅœÑœâœÉŒ∑œÇ dataset!")
        return None
    
    # ŒïŒºœÜŒ¨ŒΩŒπœÉŒ∑ metadata
    metadata = data['metadata']
    print(f"   üìä Samples: {metadata['n_samples']:,}")
    print(f"   üìä Raw features: {metadata['n_channels']} channels √ó {metadata['n_timepoints']} timepoints")
    print(f"   üìä Extracted features: {metadata['n_features']:,}")
    print(f"   üìä Classes: {metadata['n_classes']} ({', '.join(metadata['class_names'])})")
    
    # 2. Initialize Model Trainer
    print(f"\nüèãÔ∏è Œ¶Œ¨œÉŒ∑ 2: ŒëœÅœáŒπŒ∫ŒøœÄŒøŒØŒ∑œÉŒ∑ Model Trainer")
    print("-" * 50)
    
    trainer = ModelTrainer(data)
    
    # 3. Comprehensive Evaluation
    print(f"\nüöÄ Œ¶Œ¨œÉŒ∑ 3: Comprehensive Cross-Validation Evaluation")
    print("-" * 50)
    
    # ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ evaluation
    results = trainer.run_comprehensive_evaluation(cv_types=['stratified', 'subject_wise'])
    
    if not results:
        print("‚ùå ŒîŒµŒΩ œÄŒ±œÅŒ¨œáŒ∏Œ∑Œ∫Œ±ŒΩ Œ±œÄŒøœÑŒµŒªŒ≠œÉŒºŒ±œÑŒ±!")
        return None
    
    # 4. Results Analysis
    print(f"\nüìä Œ¶Œ¨œÉŒ∑ 4: Results Analysis & Visualization")
    print("-" * 50)
    
    analyzer = ResultsAnalyzer(results)
    report_data = analyzer.generate_comprehensive_report()
    
    # 5. Save Results
    print(f"\nüíæ Œ¶Œ¨œÉŒ∑ 5: ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ ŒëœÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ")
    print("-" * 50)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = save_comprehensive_results(results, report_data, timestamp)
    
    # 6. Final Summary
    print(f"\nüéâ Œ¶Œ¨œÉŒ∑ 6: Œ§ŒïŒõŒôŒöŒó ŒëŒùŒëŒ¶ŒüŒ°Œë")
    print("=" * 70)
    
    print("üèÜ TOP MODEL RANKINGS:")
    print("-" * 30)
    
    for cv_type, rankings in report_data['rankings'].items():
        print(f"\nüìä {cv_type.upper()} Cross-Validation:")
        
        # Top 3 models by composite score
        top_models = rankings['composite'][:3]
        for i, (model, score) in enumerate(top_models, 1):
            print(f"   {i}. {model}: {score:.3f}")
        
        # Best accuracy
        best_acc_model, best_acc = rankings['accuracy'][0]
        best_acc_std = results[cv_type][best_acc_model]['accuracy_std']
        print(f"   üéØ Best Accuracy: {best_acc_model} ({best_acc:.3f} ¬± {best_acc_std:.3f})")
    
    print(f"\nüìÅ ŒëŒ†ŒüŒòŒóŒöŒïŒ•ŒúŒàŒùŒë ŒëŒ°ŒßŒïŒäŒë:")
    print("-" * 30)
    for file_type, file_path in saved_files.items():
        if os.path.isfile(file_path):
            file_size = os.path.getsize(file_path) / (1024 * 1024)
            print(f"   üìÑ {os.path.basename(file_path)} ({file_size:.1f} MB)")
        else:
            print(f"   üìÅ {os.path.basename(file_path)}/ (folder)")
    
    print(f"\nüìà PERFORMANCE SUMMARY:")
    print("-" * 30)
    
    # Overall best performance
    all_accuracies = []
    for cv_type in results:
        for model in results[cv_type]:
            acc = results[cv_type][model]['accuracy_mean']
            all_accuracies.append((f"{model} ({cv_type})", acc))
    
    all_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    print(f"   ü•á Overall Best: {all_accuracies[0][0]} - {all_accuracies[0][1]:.3f}")
    print(f"   üìä Average Performance: {np.mean([acc for _, acc in all_accuracies]):.3f}")
    print(f"   üìä Performance Range: {all_accuracies[-1][1]:.3f} - {all_accuracies[0][1]:.3f}")
    
    print(f"\nüéØ ŒöŒ•Œ°ŒôŒë ŒïŒ•Œ°ŒóŒúŒëŒ§Œë:")
    print("-" * 30)
    
    # Compare ESTCNN with best baseline
    for cv_type in results:
        if 'ESTCNN' in results[cv_type]:
            estcnn_acc = results[cv_type]['ESTCNN']['accuracy_mean']
            
            # Find best baseline
            baseline_models = {k: v for k, v in results[cv_type].items() if k != 'ESTCNN'}
            if baseline_models:
                best_baseline = max(baseline_models.items(), key=lambda x: x[1]['accuracy_mean'])
                best_baseline_name, best_baseline_metrics = best_baseline
                best_baseline_acc = best_baseline_metrics['accuracy_mean']
                
                improvement = estcnn_acc - best_baseline_acc
                print(f"   üìà {cv_type}: ESTCNN vs {best_baseline_name}")
                print(f"      ESTCNN: {estcnn_acc:.3f}")
                print(f"      {best_baseline_name}: {best_baseline_acc:.3f}")
                print(f"      Improvement: {improvement:+.3f} ({improvement/best_baseline_acc*100:+.1f}%)")
    
    print(f"\nüéâ EVALUATION ŒüŒõŒüŒöŒõŒóŒ°Œ©ŒòŒóŒöŒï ŒïŒ†ŒôŒ§Œ•ŒßŒ©Œ£!")
    print("=" * 70)
    print("üìã Reports Œ∫Œ±Œπ visualizations!")
    
    return {
        'results': results,
        'analysis': report_data,
        'files': saved_files
    }


# ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ ŒºŒµ comprehensive logging
if __name__ == "__main__":
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"Comprehensive_Evaluation_Log_{timestamp}.txt"
    log_filepath = os.path.join(OUTPUT_PATH, log_filename)
    
    print(f"üöÄ ŒûŒµŒ∫ŒπŒΩŒ¨ŒµŒπ Comprehensive Model Evaluation...")
    print(f"üìÑ Log Œ∏Œ± Œ±œÄŒøŒ∏Œ∑Œ∫ŒµœÖœÑŒµŒØ: {log_filename}")
    print("=" * 70)
    
    # Comprehensive logging
    try:
        original_stdout = sys.stdout
        
        with open(log_filepath, 'w', encoding='utf-8') as f:
            class Tee:
                def __init__(self, console, file):
                    self.console = console
                    self.file = file
                def write(self, message):
                    self.console.write(message)
                    self.file.write(message)
                def flush(self):
                    self.console.flush()
                    self.file.flush()
            
            sys.stdout = Tee(original_stdout, f)
            
            # ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ main evaluation
            final_results = main()
            
            sys.stdout = original_stdout
            
        print(f"\nüìÑ Comprehensive log Œ±œÄŒøŒ∏Œ∑Œ∫ŒµœçœÑŒ∑Œ∫Œµ: {log_filename}")
        
        if final_results is not None:
            print("üéâ GRAND FINALE ŒøŒªŒøŒ∫ŒªŒ∑œÅœéŒ∏Œ∑Œ∫Œµ ŒµœÄŒπœÑœÖœáœéœÇ!")
        else:
            print("‚ùå ŒöŒ¨œÑŒπ œÄŒÆŒ≥Œµ œÉœÑœÅŒ±Œ≤Œ¨ œÉœÑŒ∑ŒΩ evaluation.")
            
    except Exception as e:
        sys.stdout = original_stdout
        print(f"‚ùå Œ£œÜŒ¨ŒªŒºŒ±: {e}")
        import traceback
        traceback.print_exc()

