# -*- coding: utf-8 -*-
"""
Î’Î—ÎœÎ‘ 3: ESTCNN & Baseline Models Implementation
===============================================

Î‘Ï…Ï„ÏŒ Ï„Î¿ script:
1. Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î¿ processed dataset Î±Ï€ÏŒ Ï„Î¿ Î²Î®Î¼Î± 2
2. Î¥Î»Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ ESTCNN model (ÎºÏ…ÏÎ¯Ï‰Ï‚ Î¼Î¿Î½Ï„Î­Î»Î¿)
3. Î¥Î»Î¿Ï€Î¿Î¹ÎµÎ¯ baseline models (PSD-SVM, LSTM, CNN variants)
4. Î•Î¾Î¬Î³ÎµÎ¹ features Î³Î¹Î± traditional ML methods
5. Î•Ï„Î¿Î¹Î¼Î¬Î¶ÎµÎ¹ training pipeline Î¼Îµ cross-validation

Î§ÏÎ®ÏƒÎ·:
Î¤ÏÎ­Î¾Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ script Î¼ÎµÏ„Î¬ Ï„Î¿ step2_preprocessing_epoching.py
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import pickle
import warnings
warnings.filterwarnings('ignore')

# Deep Learning & ML
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    print(f"âœ… TensorFlow {tf.__version__} Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
except ImportError:
    print("âŒ Î£Î¦Î‘Î›ÎœÎ‘: Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î±: pip install tensorflow")
    exit()

try:
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import cross_val_score, StratifiedKFold
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    import pandas as pd
    print("âœ… Scikit-learn & pandas Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
except ImportError:
    print("âŒ Î£Î¦Î‘Î›ÎœÎ‘: Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î±: pip install scikit-learn pandas")
    exit()

try:
    from scipy import signal
    from scipy.stats import skew, kurtosis
    print("âœ… SciPy Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
except ImportError:
    print("âŒ Î£Î¦Î‘Î›ÎœÎ‘: Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î±: pip install scipy")
    exit()

# OUTPUT PATH
OUTPUT_PATH = r"C:\Users\nikos22594\python_code"

class ESTCNNModel:
    """
    EEG-based Spatio-Temporal Convolutional Neural Network (ESTCNN)
    
    Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÏ„Î¿ paper: "EEG-Based Spatioâ€“Temporal Convolutional Neural 
    Network for Driver Fatigue Evaluation" by Gao et al.
    """
    
    def __init__(self, input_shape=(30, 100, 1), num_classes=2):
        """
        Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· ESTCNN
        
        Args:
            input_shape: (channels, timepoints, features) = (30, 100, 1)
            num_classes: 2 (Alert vs Fatigue)
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        
    def create_core_block(self, inputs, filters, pool_size, pool_type='max', block_name="core"):
        """
        Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Core Block ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Ï„Î¿ paper
        
        Core Block = 3Ã—(Conv1D + ReLU + BatchNorm) + Pooling
        """
        x = inputs
        
        # 3 Convolutional layers Î¼Îµ kernel size 3
        for i in range(3):
            x = layers.Conv1D(
                filters=filters,
                kernel_size=3,
                padding='valid',
                activation='relu',
                name=f'{block_name}_conv_{i+1}'
            )(x)
            x = layers.BatchNormalization(name=f'{block_name}_bn_{i+1}')(x)
        
        # Pooling layer
        if pool_type == 'max':
            x = layers.MaxPooling1D(
                pool_size=pool_size,
                name=f'{block_name}_maxpool'
            )(x)
        else:  # average pooling
            x = layers.AveragePooling1D(
                pool_size=pool_size,
                name=f'{block_name}_avgpool'
            )(x)
        
        return x
    
    def build_model(self):
        """
        ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Ï„Î¿Ï… ESTCNN Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Ï„Î¿ paper
        
        Architecture:
        - Core Block 1: filters=16, max pooling (pool_size=2)
        - Core Block 2: filters=32, max pooling (pool_size=2)  
        - Core Block 3: filters=64, avg pooling (pool_size=7)
        - Dense Layer: 50 neurons
        - Output Layer: 2 neurons (softmax)
        """
        print("ğŸ§  ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶Ï‰ ESTCNN model...")
        
        # Input layer: (None, 30, 100, 1) Î³Î¹Î± CNN2D Î® (None, 100, 30) Î³Î¹Î± CNN1D
        # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ CNN1D Î³Î¹Î± temporal convolutions
        inputs = keras.Input(shape=(100, 30), name='eeg_input')  # (timepoints, channels)
        
        print(f"   ğŸ“Š Input shape: {inputs.shape}")
        
        # Core Block 1: 16 filters, max pooling size 2
        x = self.create_core_block(inputs, filters=16, pool_size=2, 
                                 pool_type='max', block_name='core1')
        print(f"   ğŸ”§ After Core Block 1: temporal dim reduced by ~4x")
        
        # Core Block 2: 32 filters, max pooling size 2
        x = self.create_core_block(x, filters=32, pool_size=2, 
                                 pool_type='max', block_name='core2')
        print(f"   ğŸ”§ After Core Block 2: temporal dim reduced further")
        
        # Core Block 3: 64 filters, average pooling size 7
        x = self.create_core_block(x, filters=64, pool_size=7, 
                                 pool_type='avg', block_name='core3')
        print(f"   ğŸ”§ After Core Block 3: temporal features extracted")
        
        # Flatten Î³Î¹Î± dense layers (spatial feature fusion)
        x = layers.Flatten(name='flatten')(x)
        
        # Dense layer Î³Î¹Î± spatial feature fusion
        x = layers.Dense(50, activation='relu', name='dense_spatial')(x)
        x = layers.Dropout(0.5, name='dropout')(x)
        
        # Output layer
        outputs = layers.Dense(self.num_classes, activation='softmax', name='output')(x)
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        self.model = keras.Model(inputs=inputs, outputs=outputs, name='ESTCNN')
        
        print(f"   âœ… ESTCNN model Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
        
        return self.model
    
    def compile_model(self, learning_rate=0.001):
        """
        Compile Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î¼Îµ optimizer ÎºÎ±Î¹ loss function
        """
        if self.model is None:
            self.build_model()
        
        # SGD optimizer ÏŒÏ€Ï‰Ï‚ ÏƒÏ„Î¿ paper
        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
        
        self.model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"   âœ… ESTCNN compiled Î¼Îµ SGD optimizer (lr={learning_rate})")
    
    def summary(self):
        """
        Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· summary Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        """
        if self.model is None:
            self.build_model()
        
        print("\nğŸ“‹ ESTCNN Model Architecture:")
        print("=" * 50)
        self.model.summary()
        return self.model


class BaselineModels:
    """
    Baseline Î¼Î¿Î½Ï„Î­Î»Î± Î³Î¹Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼Îµ Ï„Î¿ ESTCNN
    """
    
    @staticmethod
    def create_simple_cnn():
        """
        Î‘Ï€Î»ÏŒ CNN baseline
        """
        model = models.Sequential([
            layers.Conv1D(32, 7, activation='relu', input_shape=(100, 30)),
            layers.MaxPooling1D(2),
            layers.Conv1D(64, 5, activation='relu'),
            layers.MaxPooling1D(2),
            layers.Conv1D(128, 3, activation='relu'),
            layers.GlobalAveragePooling1D(),
            layers.Dense(50, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(2, activation='softmax')
        ], name='Simple_CNN')
        
        model.compile(optimizer='adam', 
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])
        return model
    
    @staticmethod
    def create_lstm_model():
        """
        LSTM model Î³Î¹Î± temporal dependencies
        """
        model = models.Sequential([
            layers.LSTM(64, return_sequences=True, input_shape=(100, 30)),
            layers.Dropout(0.3),
            layers.LSTM(32, return_sequences=False),
            layers.Dropout(0.3),
            layers.Dense(50, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(2, activation='softmax')
        ], name='LSTM_Model')
        
        model.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy', 
                     metrics=['accuracy'])
        return model
    
    @staticmethod
    def create_cnn_lstm_hybrid():
        """
        CNN-LSTM hybrid model
        """
        model = models.Sequential([
            layers.Conv1D(32, 5, activation='relu', input_shape=(100, 30)),
            layers.MaxPooling1D(2),
            layers.Conv1D(64, 3, activation='relu'),
            layers.MaxPooling1D(2),
            layers.LSTM(32, return_sequences=False),
            layers.Dense(50, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(2, activation='softmax')
        ], name='CNN_LSTM_Hybrid')
        
        model.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])
        return model


class FeatureExtractor:
    """
    Î•Î¾Î±Î³Ï‰Î³Î® features Î³Î¹Î± traditional ML methods
    """
    
    def __init__(self, sfreq=100):
        self.sfreq = sfreq
        
    def extract_statistical_features(self, epochs):
        """
        Î•Î¾Î±Î³Ï‰Î³Î® ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½
        
        Args:
            epochs: (n_epochs, n_channels, n_timepoints)
            
        Returns:
            features: (n_epochs, n_features)
        """
        print("   ğŸ“Š Î•Î¾Î±Î³Ï‰Î³Î® ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ features...")
        
        n_epochs, n_channels, n_timepoints = epochs.shape
        features_list = []
        
        for epoch in epochs:
            epoch_features = []
            
            for ch in range(n_channels):
                signal_ch = epoch[ch, :]
                
                # Î’Î±ÏƒÎ¹ÎºÎ¬ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
                mean_val = np.mean(signal_ch)
                std_val = np.std(signal_ch)
                var_val = np.var(signal_ch)
                
                # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Î½ÏÏ„ÎµÏÎ·Ï‚ Ï„Î¬Î¾Î·Ï‚
                skew_val = skew(signal_ch)
                kurt_val = kurtosis(signal_ch)
                
                # Min/Max values
                min_val = np.min(signal_ch)
                max_val = np.max(signal_ch)
                
                # Range
                range_val = max_val - min_val
                
                epoch_features.extend([mean_val, std_val, var_val, skew_val, 
                                     kurt_val, min_val, max_val, range_val])
            
            features_list.append(epoch_features)
        
        features = np.array(features_list)
        print(f"      âœ… Statistical features shape: {features.shape}")
        return features
    
    def extract_spectral_features(self, epochs):
        """
        Î•Î¾Î±Î³Ï‰Î³Î® Ï†Î±ÏƒÎ¼Î±Ï„Î¹ÎºÏÎ½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (PSD, band powers)
        """
        print("   ğŸ“Š Î•Î¾Î±Î³Ï‰Î³Î® spectral features...")
        
        n_epochs, n_channels, n_timepoints = epochs.shape
        features_list = []
        
        # ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ frequency bands
        bands = {
            'delta': (1, 4),
            'theta': (4, 8), 
            'alpha': (8, 13),
            'beta': (13, 30),
            'gamma': (30, 50)
        }
        
        for epoch in epochs:
            epoch_features = []
            
            for ch in range(n_channels):
                signal_ch = epoch[ch, :]
                
                # Power Spectral Density
                freqs, psd = signal.welch(signal_ch, fs=self.sfreq, 
                                        nperseg=min(64, len(signal_ch)))
                
                # Band powers
                for band_name, (low_freq, high_freq) in bands.items():
                    band_mask = (freqs >= low_freq) & (freqs <= high_freq)
                    band_power = np.mean(psd[band_mask])
                    epoch_features.append(band_power)
                
                # Spectral statistics
                dominant_freq = freqs[np.argmax(psd)]
                spectral_centroid = np.sum(freqs * psd) / np.sum(psd)
                spectral_rolloff = freqs[np.where(np.cumsum(psd) >= 0.85 * np.sum(psd))[0][0]]
                
                epoch_features.extend([dominant_freq, spectral_centroid, spectral_rolloff])
            
            features_list.append(epoch_features)
        
        features = np.array(features_list)
        print(f"      âœ… Spectral features shape: {features.shape}")
        return features
    
    def extract_all_features(self, epochs):
        """
        Î•Î¾Î±Î³Ï‰Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ features
        """
        print("ğŸ“Š Î•Î¾Î±Î³Ï‰Î³Î® Ï€Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÏÎ½ features Î³Î¹Î± ML...")
        
        # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ features
        stat_features = self.extract_statistical_features(epochs)
        
        # Î¦Î±ÏƒÎ¼Î±Ï„Î¹ÎºÎ¬ features
        spectral_features = self.extract_spectral_features(epochs)
        
        # Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ features
        all_features = np.concatenate([stat_features, spectral_features], axis=1)
        
        print(f"âœ… Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ features: {all_features.shape}")
        return all_features


def load_processed_dataset(dataset_path=None):
    """
    Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… processed dataset Î±Ï€ÏŒ Ï„Î¿ Î²Î®Î¼Î± 2
    """
    print("ğŸ“‚ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· processed dataset...")
    
    if dataset_path is None:
        # Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Ï„Î¿Ï… Ï€Î¹Î¿ Ï€ÏÏŒÏƒÏ†Î±Ï„Î¿Ï… dataset
        files = [f for f in os.listdir(OUTPUT_PATH) if f.startswith('processed_eeg_dataset_')]
        if not files:
            print("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ processed dataset!")
            return None
        
        # Î Î¬ÏÎµ Ï„Î¿ Ï€Î¹Î¿ Ï€ÏÏŒÏƒÏ†Î±Ï„Î¿
        latest_file = sorted(files)[-1]
        dataset_path = os.path.join(OUTPUT_PATH, latest_file)
    
    print(f"   ğŸ“ Î¦Î¿ÏÏ„ÏÎ½Ï‰: {os.path.basename(dataset_path)}")
    
    try:
        with open(dataset_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"   âœ… Dataset Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
        return data
    
    except Exception as e:
        print(f"   âŒ Î£Ï†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚: {e}")
        return None


def prepare_data_for_models(dataset_dict):
    """
    Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î±
    """
    print("ğŸ”§ Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± models...")
    
    # Î•Î¾Î±Î³Ï‰Î³Î® epochs ÎºÎ±Î¹ labels Î±Ï€ÏŒ ÏŒÎ»Î± Ï„Î± subjects
    all_epochs = []
    all_labels = []
    subject_info = []
    
    for subject_name, subject_data in dataset_dict['dataset'].items():
        epochs = subject_data['epochs']
        labels = subject_data['labels']
        
        all_epochs.append(epochs)
        all_labels.append(labels)
        
        # Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± cross-validation
        subject_indices = [subject_name] * len(epochs)
        subject_info.extend(subject_indices)
        
        print(f"   ğŸ“Š {subject_name}: {len(epochs)} epochs")
    
    # Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    X = np.concatenate(all_epochs, axis=0)  # (total_epochs, 30, 100)
    y = np.concatenate(all_labels, axis=0)  # (total_epochs,)
    subjects = np.array(subject_info)       # (total_epochs,) Î¼Îµ subject names
    
    print(f"âœ… Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±:")
    print(f"   ğŸ“¦ X shape: {X.shape}")
    print(f"   ğŸ·ï¸  y shape: {y.shape}")
    print(f"   ğŸ‘¥ Subjects: {len(np.unique(subjects))}")
    print(f"   ğŸ“Š Class distribution: {np.bincount(y)}")
    
    return X, y, subjects


def main():
    """
    ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· - Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½
    """
    print("ğŸ§  MODEL IMPLEMENTATION & FEATURE EXTRACTION - Î’Î—ÎœÎ‘ 3")
    print("=" * 70)
    print(f"ğŸ“… Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· processed dataset
    print(f"\nğŸ“‚ Î¦Î¬ÏƒÎ· 1: Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Dataset")
    print("-" * 40)
    
    dataset = load_processed_dataset()
    if dataset is None:
        print("âŒ Î‘Î´Ï…Î½Î±Î¼Î¯Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ dataset!")
        return None
    
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· metadata
    metadata = dataset['metadata']
    print(f"   ğŸ“Š Total subjects: {metadata['total_subjects']}")
    print(f"   ğŸ“Š Total epochs: {metadata['total_epochs']:,}")
    print(f"   ğŸ“Š Alert epochs: {metadata['alert_epochs']:,}")
    print(f"   ğŸ“Š Fatigue epochs: {metadata['fatigue_epochs']:,}")
    
    # 2. Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    print(f"\nğŸ”§ Î¦Î¬ÏƒÎ· 2: Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
    print("-" * 40)
    
    X, y, subjects = prepare_data_for_models(dataset)
    
    # 3. Feature extraction Î³Î¹Î± traditional ML
    print(f"\nğŸ“Š Î¦Î¬ÏƒÎ· 3: Feature Extraction")
    print("-" * 40)
    
    feature_extractor = FeatureExtractor(sfreq=100)
    traditional_features = feature_extractor.extract_all_features(X)
    
    # 4. ESTCNN Model Implementation
    print(f"\nğŸ§  Î¦Î¬ÏƒÎ· 4: ESTCNN Model Implementation")
    print("-" * 50)
    
    estcnn = ESTCNNModel(input_shape=(100, 30), num_classes=2)
    estcnn_model = estcnn.build_model()
    estcnn.compile_model(learning_rate=0.001)
    estcnn.summary()
    
    # 5. Baseline Models Implementation
    print(f"\nğŸ—ï¸  Î¦Î¬ÏƒÎ· 5: Baseline Models Implementation")
    print("-" * 50)
    
    print("   ğŸ”§ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏÎ½Ï„Î±Ï‚ baseline models...")
    
    # Deep Learning baselines
    simple_cnn = BaselineModels.create_simple_cnn()
    lstm_model = BaselineModels.create_lstm_model()
    cnn_lstm_hybrid = BaselineModels.create_cnn_lstm_hybrid()
    
    print("   âœ… Simple CNN model")
    print("   âœ… LSTM model") 
    print("   âœ… CNN-LSTM hybrid model")
    
    # Traditional ML models
    print("   ğŸ”§ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏÎ½Ï„Î±Ï‚ traditional ML models...")
    
    svm_model = SVC(kernel='rbf', random_state=42)
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    print("   âœ… SVM model")
    print("   âœ… Random Forest model")
    
    # 6. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎºÎ±Î¹ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    print(f"\nğŸ’¾ Î¦Î¬ÏƒÎ· 6: Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Models & Data")
    print("-" * 40)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± training
    ml_data = {
        'X_raw': X,  # Raw epochs Î³Î¹Î± deep learning
        'X_features': traditional_features,  # Extracted features Î³Î¹Î± ML
        'y': y,
        'subjects': subjects,
        'metadata': {
            'n_samples': len(X),
            'n_channels': X.shape[1],
            'n_timepoints': X.shape[2],
            'n_features': traditional_features.shape[1],
            'n_classes': len(np.unique(y)),
            'class_names': ['Alert', 'Fatigue'],
            'sampling_rate': 100,
            'created': datetime.now().isoformat()
        }
    }
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    data_filename = f"ml_ready_data_{timestamp}.pkl"
    data_filepath = os.path.join(OUTPUT_PATH, data_filename)
    
    with open(data_filepath, 'wb') as f:
        pickle.dump(ml_data, f)
    
    print(f"âœ… ML data Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½: {data_filename}")
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ESTCNN model
    estcnn_filename = f"estcnn_model_{timestamp}.h5"
    estcnn_filepath = os.path.join(OUTPUT_PATH, estcnn_filename)
    estcnn_model.save(estcnn_filepath)
    
    print(f"âœ… ESTCNN model Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {estcnn_filename}")
    
    # 7. Î¤ÎµÎ»Î¹ÎºÎ¬ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
    print(f"\nğŸ“ˆ Î¦Î¬ÏƒÎ· 7: Î¤ÎµÎ»Î¹ÎºÎ¬ Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬")
    print("=" * 50)
    
    print(f"ğŸ¯ ÎˆÏ„Î¿Î¹Î¼Î± Models:")
    print(f"   âœ… ESTCNN (ÎºÏÏÎ¹Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿)")
    print(f"   âœ… Simple CNN (baseline)")
    print(f"   âœ… LSTM (baseline)")
    print(f"   âœ… CNN-LSTM Hybrid (baseline)")
    print(f"   âœ… SVM (traditional ML)")
    print(f"   âœ… Random Forest (traditional ML)")
    
    print(f"\nğŸ“Š Dataset Statistics:")
    print(f"   ğŸ“¦ Raw data shape: {X.shape}")
    print(f"   ğŸ“¦ Features shape: {traditional_features.shape}")
    print(f"   ğŸ‘¥ Subjects: {len(np.unique(subjects))}")
    print(f"   ğŸ·ï¸  Classes: {len(np.unique(y))} (Alert: {np.sum(y==0)}, Fatigue: {np.sum(y==1)})")
    
    print(f"\nğŸ’¾ Î‘Ï€Î¿Î¸Î·ÎºÎµÏ…Î¼Î­Î½Î± Î‘ÏÏ‡ÎµÎ¯Î±:")
    print(f"   ğŸ“ {data_filename} ({os.path.getsize(data_filepath)/(1024*1024):.1f} MB)")
    print(f"   ğŸ“ {estcnn_filename} ({os.path.getsize(estcnn_filepath)/(1024*1024):.1f} MB)")
    
    print(f"\nğŸ‰ ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î©Î£Î— Î•Î Î™Î¤Î¥Î§ÎŸÎ¥Î£!")
    print("=" * 50)
    print("âœ… ÎŒÎ»Î± Ï„Î± models ÎµÎ¯Î½Î±Î¹ Î­Ï„Î¿Î¹Î¼Î± Î³Î¹Î± training!")
    print("ğŸ“‹ Î•Ï€ÏŒÎ¼ÎµÎ½Î¿ Î²Î®Î¼Î±: Cross-validation & evaluation")
    
    return {
        'models': {
            'estcnn': estcnn_model,
            'simple_cnn': simple_cnn,
            'lstm': lstm_model,
            'cnn_lstm': cnn_lstm_hybrid,
            'svm': svm_model,
            'random_forest': rf_model
        },
        'data': ml_data,
        'filepaths': {
            'data': data_filepath,
            'estcnn': estcnn_filepath
        }
    }


# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î¼Îµ report generation
if __name__ == "__main__":
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"Model_Implementation_Report_{timestamp}.txt"
    report_filepath = os.path.join(OUTPUT_PATH, report_filename)
    
    print(f"ğŸš€ ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ Model Implementation Pipeline...")
    print(f"ğŸ“„ Report Î¸Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏ…Ï„ÎµÎ¯: {report_filename}")
    print("=" * 70)
    
    # Output capture
    try:
        original_stdout = sys.stdout
        
        with open(report_filepath, 'w', encoding='utf-8') as f:
            class Tee:
                def __init__(self, console, file):
                    self.console = console
                    self.file = file
                def write(self, message):
                    self.console.write(message)
                    self.file.write(message)
                def flush(self):
                    self.console.flush()
                    self.file.flush()
            
            sys.stdout = Tee(original_stdout, f)
            result = main()
            sys.stdout = original_stdout
            
        print(f"\nğŸ“„ Report Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {report_filename}")
        
        if result is not None:
            print("ğŸ‰ Model implementation Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
            print("ğŸš€ ÎˆÏ„Î¿Î¹Î¼Î¿ Î³Î¹Î± training ÎºÎ±Î¹ evaluation!")
        else:
            print("âŒ ÎšÎ¬Ï„Î¹ Ï€Î®Î³Îµ ÏƒÏ„ÏÎ±Î²Î¬ ÏƒÏ„Î· model implementation.")
            
    except Exception as e:
        sys.stdout = original_stdout
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î±: {e}")
